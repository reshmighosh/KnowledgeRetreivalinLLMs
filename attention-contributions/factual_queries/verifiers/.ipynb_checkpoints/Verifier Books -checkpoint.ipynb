{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d7f60-209f-4464-a8a6-0507b36d2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "engine = \"GPT35\"\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_chain,\n",
    "    wait_fixed\n",
    ") \n",
    "\n",
    "@retry(wait=wait_chain(*[wait_fixed(3) for i in range(3)] +\n",
    "                       [wait_fixed(5) for i in range(2)] +\n",
    "                       [wait_fixed(10)]))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)\n",
    "\n",
    "\n",
    "def verify_entity_exists(response, list_in_txt):\n",
    "    system_prompt = \"You are an AI assistant that helps the user verify whether an entity is captured in a list.\"\n",
    "    prompt = \"\"\"I will give you one entity and one list, and I want you to respond with \\\"YES\\\" if the entity is within the list, \\\"NO\\\" if it is not in the list.  \n",
    "    \n",
    "    List: \n",
    "    {}\n",
    "    Entity: {}\n",
    "    Give your response in the following format: \n",
    "    `Reference in the list: {{item in the list if exists, None otherwise}}\n",
    "    Answer: {{YES or NO}}` and say nothing else.\n",
    "    \"\"\"\n",
    "    response = completion_with_backoff(\n",
    "      engine=engine,\n",
    "      messages = [{\"role\":\"system\",\"content\":system_prompt},\n",
    "                  {\"role\":\"user\",\"content\":prompt.format(list_in_txt, response)},],\n",
    "      temperature=0,\n",
    "      max_tokens=25,\n",
    "      top_p=0.95,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0,\n",
    "      stop=None)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        lines = result.split(\"\\n\")\n",
    "        reference = lines[0].split(\": \")[1]\n",
    "        answer = lines[1].split(\": \")[1]\n",
    "        return reference, answer\n",
    "    except:\n",
    "        print(prompt.format(list_in_txt, response))\n",
    "        print(\"--------------------------------\")\n",
    "        return None, None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_output(records, correctness, output_file):\n",
    "    records_final = []\n",
    "\n",
    "    for i in range(len(records['prompt'])):\n",
    "        prompt = records['prompt'][i]\n",
    "        completion = records['completion'][i]\n",
    "        record = (prompt, completion, [correctness[i][0], correctness[i][1]])\n",
    "        records_final.append(record)\n",
    "\n",
    "    # Save records_final as a pkl file\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(records_final, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def query_wikidata(book_name):\n",
    "    book_name = book_name.replace('\"', '')\n",
    "    url = 'https://query.wikidata.org/sparql'    \n",
    "    query = '''\n",
    "       SELECT ?bookLabel ?authorLabel (YEAR(?publicationDate) as ?publishing_year) WHERE {{\n",
    "       ?book rdfs:label \"{0}\"@en .\n",
    "       ?book wdt:P50 ?author .\n",
    "       ?book wdt:P577 ?publicationDate .\n",
    "  \n",
    "       SERVICE wikibase:label {{\n",
    "          bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" .\n",
    "       }}\n",
    "    }}\n",
    "    '''.format(book_name)\n",
    "        \n",
    "    # Pass the query as a URL-encoded string in the params parameter\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'query': query\n",
    "    }\n",
    "    \n",
    "    # Make the HTTP GET request\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "def organize_data(data):\n",
    "    organized_data = []\n",
    "    \n",
    "    try:\n",
    "        if data is not None:\n",
    "            for item in data['results']['bindings']:\n",
    "                person = item['authorLabel']['value']\n",
    "                publishing_year = item['publishing_year']['value']\n",
    "                book = item['bookLabel']['value']\n",
    "\n",
    "                organized_data.append({\n",
    "                    \"Author\": person,\n",
    "                    \"Publishing_Year\" : publishing_year,\n",
    "                    \"Book\": book,\n",
    "                })\n",
    "    except:\n",
    "        print(\"wiki extraction failed\")\n",
    "        \n",
    "    return organized_data\n",
    "\n",
    "def get_book_info(book_name):\n",
    "    book_data = query_wikidata(book_name)\n",
    "    organized_data = organize_data(book_data)\n",
    "\n",
    "    matching_authors = []\n",
    "    matching_publishing_years = []\n",
    "    for entry in organized_data:\n",
    "\n",
    "        if entry['Book'].lower().strip() == book_name.lower().strip():\n",
    "            if entry['Author'] not in matching_authors:\n",
    "                matching_authors.append(entry['Author'])\n",
    "                \n",
    "            if entry['Publishing_Year'] not in matching_publishing_years:   \n",
    "                matching_publishing_years.append(entry['Publishing_Year'])\n",
    "    \n",
    "    return matching_authors, matching_publishing_years\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07974b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def remove_delimiter(text):\n",
    "    text = text.replace('\"','')\n",
    "    delimiters = ['.', ',']\n",
    "    for delimiter in delimiters:\n",
    "        if text.endswith(delimiter):\n",
    "            return text[:-len(delimiter)]        \n",
    "    return text\n",
    "\n",
    "def verify_books(records, debug_file):\n",
    "    \n",
    "    correctness = np.zeros((len(records[\"prompt\"]), 2))\n",
    "    with open(debug_file,\"w\") as fd:\n",
    "        for i in range(len(records[\"prompt\"])):\n",
    "            time.sleep(5)\n",
    "\n",
    "            constraints = records[\"name\"][i]\n",
    "            completion = records[\"completion\"][i]\n",
    "            book_name = remove_delimiter(completion.split(\"\\n\")[0].strip())\n",
    "            fd.write(f\"Book name from the completion : {book_name}\"+ \"\\n\")\n",
    "            matching_authors, matching_publishing_years = get_book_info(book_name)    \n",
    "\n",
    "            fd.write(\"wiki data:\" + \"\\n\")\n",
    "            fd.write(f\"author : {matching_authors}\"+ \"\\n\")\n",
    "            fd.write(f\"publishing year : {matching_publishing_years}\"+ \"\\n\")\n",
    "            fd.write(\"..................................\\n\")\n",
    "\n",
    "            for constraint in constraints:        \n",
    "                if \"written by\" in constraint and len(matching_authors) > 0 :\n",
    "                    state_reference, state_answer = verify_entity_exists(constraint, matching_authors)\n",
    "\n",
    "                    if state_answer is not None:\n",
    "                        correctness[i][0] = (1 if state_answer.lower() == \"yes\" else 0)\n",
    "                    fd.write(f\"author constraint : {constraint} \\n\")\n",
    "                    fd.write(f\"Turbo author reference : {state_reference} \\n\")\n",
    "                    fd.write(f\"Turbo author answer : {state_answer} \\n\")\n",
    "                    fd.write(\".................................\\n\")\n",
    "\n",
    "                else:\n",
    "                    if len(matching_publishing_years) > 0 :\n",
    "                        #correctness[i][0]  - holds the awards constraint outcome\n",
    "                        state_reference, state_answer = verify_entity_exists(constraint, matching_publishing_years)\n",
    "                        if state_answer is not None:\n",
    "                            correctness[i][1] = (1 if state_answer.lower() == \"yes\" else 0)\n",
    "                        fd.write(f\"publishing year constraint: {constraint}\"+ \"\\n\")\n",
    "                        fd.write(f\"Turbo publishing year reference : {state_reference}\"+ \"\\n\")\n",
    "                        fd.write(f\"Turbo publishing year answer : {state_answer} \\n\")\n",
    "                        fd.write(\"..............................\\n\")\n",
    "            fd.write(\"===============================================\\n\")\n",
    "    return correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145aa6d-5ec8-4cca-98ec-adcc61ca82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "data_pretty = {\n",
    "    \"books\": \"Books\",   \n",
    "}\n",
    "result_records = []\n",
    "for model_size in [\"7b\", \"13b\", \"70b\"]:\n",
    "    for data_name in data_pretty:\n",
    "        filename = f\"./outputs/Llama-2-{model_size}-hf_{data_name}_localized_track.pkl\"\n",
    "        output_file = f\"./outputs/Llama-2-{model_size}-hf_{data_name}_localized_track.pkl\"\n",
    "        debug_file = f\".outputs/Llama-2-{model_size}-hf_{data_name}_localized_track.debug.txt\"\n",
    "\n",
    "              \n",
    "        if not os.path.exists(filename):\n",
    "            print(filename)\n",
    "            continue\n",
    "        records_to_save = edict(pickle.load(open(filename, \"rb\")))\n",
    "        records = records_to_save\n",
    "        correctness = verify_books(records, debug_file) \n",
    "        save_output(records, correctness, output_file )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6a16e-f8a8-464d-ade1-03f4cdbbee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally \n",
    "## records = [(prompt, completion, [0, 0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
