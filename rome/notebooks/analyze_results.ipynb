{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(factfile):\n",
    "        gts = []\n",
    "        preds = []\n",
    "        records = []\n",
    "        with jsonlines.open(factfile) as reader:\n",
    "            for line in reader:\n",
    "                if isinstance(line['prediction'], str):\n",
    "                    p = line['prediction'].lower().strip()\n",
    "                else:\n",
    "                    p = [pred.lower().strip() for pred in line['prediction']]\n",
    "                if isinstance(line['correct'], str):\n",
    "                    c = line['correct'].lower().strip()\n",
    "                else:\n",
    "                    c = [corr.lower().strip() for corr in line['correct']]\n",
    "                preds.append(p)\n",
    "                gts.append(c)\n",
    "                records.append(line)\n",
    "        return preds,gts,records\n",
    "    \n",
    "def accuracy(preds, gts):\n",
    "    ncorrect = 0\n",
    "    for p,g in zip(preds, gts):\n",
    "        if isinstance(p, str):\n",
    "            if p == g:\n",
    "                ncorrect+=1\n",
    "        else:\n",
    "            if g in p:\n",
    "                ncorrect+=1\n",
    "    return ncorrect/len(preds) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_results(factfile):\n",
    "    preds,gts,_ = get_results(factfile)\n",
    "    return accuracy(preds, gts)\n",
    "\n",
    "def get_incorrect_preds(factfile):\n",
    "    preds, gts, records = get_results(factfile)\n",
    "    incorrect_records = []\n",
    "    for record_id, (pred,gt,record) in enumerate(zip(preds, gts, records)):\n",
    "        if pred != gt:\n",
    "            incorrect_records.append((record_id, record))\n",
    "    return incorrect_records\n",
    "\n",
    "def get_all_records(factfile):\n",
    "    _, _, records = get_results(factfile)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"mistral\", \"gemma\", \"phi\", \"falcon\", \"llama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_accuracy_in_topk(modelname, k=5):\n",
    "    acc = get_accuracy_from_results(f\"predictions_{modelname}_top{k}.jsonl\")\n",
    "    return acc\n",
    "\n",
    "def analyze_accuracy(filename):\n",
    "    acc = get_accuracy_from_results(filename)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.52542372881356"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_accuracy(\"results_instructed_augmented_llama.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.66101694915254"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_accuracy(\"results_augmented_llama.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_accuracy(\"results_instructed_augmented_llama.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.02325581395348"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_accuracy('results_augmented_mistral.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.67441860465115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_accuracy('results_instructed_augmented_mistral.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral  95.33622559652929\n",
      "gemma  93.87417218543047\n",
      "phi  88.17204301075269\n",
      "falcon  93.255620316403\n",
      "llama  93.45898004434589\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    print(f\"{m}  {analyze_accuracy_in_topk(m)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_records(filename):\n",
    "    with jsonlines.open(filename) as reader:\n",
    "        augmented_records = []\n",
    "        for i,m in enumerate(reader):\n",
    "            augmented_records.append(m)\n",
    "\n",
    "            # refer to original dataset with id available\n",
    "            idx = m['known_id']\n",
    "            subject = original_dataset[idx]['subject']\n",
    "            attribute = original_dataset[idx]['attribute']\n",
    "            prompt = augmented_records[i]['prompt']\n",
    "\n",
    "            augmented_records[i]['subject'] = f\"{attribute}\"\n",
    "            augmented_records[i]['prompt'] = f\"{prompt} {attribute}. {prompt}\"\n",
    "        return augmented_records\n",
    "\n",
    "\n",
    "def dump_to_file(records, modelname, filename):\n",
    "    with jsonlines.open(f\"{filename}_{modelname}.jsonl\", 'w') as writer:\n",
    "        for r in records:\n",
    "            writer.write(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = json.load(open(\"../notebooks/data/known_1000.json\"))\n",
    "def get_incorrectly_done_records(modelname):\n",
    "    preds, gts, records = get_results(f\"predictions_{modelname}_top5.jsonl\")\n",
    "    incorrect_records = []\n",
    "    for record_id, (pred,gt,record) in enumerate(zip(preds, gts, records)):\n",
    "        if gt not in pred:  \n",
    "            record['subject'] = original_dataset[record['known_id']]['subject']\n",
    "            record['clean_subject'] = record['subject']\n",
    "            record['subject'] = ''.join(record['subject'].split())\n",
    "            incorrect_records.append(record)\n",
    "    return incorrect_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_incorrect = get_incorrectly_done_records('mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_incorrect = get_incorrectly_done_records('llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = json.load(open(\"../notebooks/data/known_1000.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_file(mistral_incorrect, 'mistral', 'incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_file(llama_incorrect, 'llama', 'incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"augmented_llama.jsonl\", \"w\") as writer:\n",
    "    with jsonlines.open(\"incorrect_llama.jsonl\") as reader:\n",
    "        for line in reader:\n",
    "            line['prompt'] = f\"{line['prompt']} {line['correct']}. {line['prompt']}\"\n",
    "            line['subject'] = line['correct']\n",
    "            writer.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"augmented_mistral.jsonl\", \"w\") as writer:\n",
    "    with jsonlines.open(\"incorrect_mistral.jsonl\") as reader:\n",
    "        for line in reader:\n",
    "            line['prompt'] = f\"{line['prompt']} {line['correct']}. {line['prompt']}\"\n",
    "            line['subject'] = line['correct']\n",
    "            writer.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with jsonlines.open(\"incorrect_multidoc_mistral.jsonl\", \"w\") as writer:\n",
    "    with jsonlines.open(\"incorrect_mistral.jsonl\") as reader:\n",
    "        all_prompts = []\n",
    "        for line in reader:\n",
    "            all_prompts.append(line)\n",
    "        for i, line in enumerate(all_prompts):\n",
    "            current_context = f\"{line['prompt']} {line['correct']}\"\n",
    "            random_context_1, random_context_2, random_context_3 = tuple(random.sample(all_prompts[:i]+all_prompts[i+1:], 3))\n",
    "            random_context_1 = f\"{random_context_1['prompt']} {random_context_1['correct']}\"\n",
    "            random_context_2 = f\"{random_context_2['prompt']} {random_context_2['correct']}\"\n",
    "            random_context_3 = f\"{random_context_3['prompt']} {random_context_3['correct']}\"\n",
    "            full_context = [current_context, random_context_1, random_context_2, random_context_3]\n",
    "            random.shuffle(full_context)\n",
    "            full_context = \"\\n\".join(full_context)\n",
    "            line['prompt'] = f\"Answer based on the context. Context: {full_context}\" + \\\n",
    "                                \"\\n\" + \\\n",
    "                                f\"Question: {line['prompt']}\"\n",
    "            line['subject'] = line['correct']\n",
    "            writer.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with jsonlines.open(\"incorrect_multidoc_mistral.jsonl\", \"w\") as writer:\n",
    "    with jsonlines.open(\"incorrect_mistral.jsonl\") as reader:\n",
    "        all_prompts = []\n",
    "        for line in reader:\n",
    "            all_prompts.append(line)\n",
    "        for i, line in enumerate(all_prompts):\n",
    "            current_context = f\"{line['prompt']} {line['correct']}\"\n",
    "            random_context_1, random_context_2, random_context_3 = tuple(random.sample(all_prompts[:i]+all_prompts[i+1:], 3))\n",
    "            random_context_1 = f\"{random_context_1['prompt']} {random_context_1['correct']}\"\n",
    "            random_context_2 = f\"{random_context_2['prompt']} {random_context_2['correct']}\"\n",
    "            random_context_3 = f\"{random_context_3['prompt']} {random_context_3['correct']}\"\n",
    "            full_context = [current_context, random_context_1, random_context_2, random_context_3]\n",
    "            random.shuffle(full_context)\n",
    "            full_context = \"\\n\".join(full_context)\n",
    "            line['prompt'] = f\"Answer based on the context. Context: {full_context}\" + \\\n",
    "                                \"\\n\" + \\\n",
    "                                f\"Question: {line['prompt']}\"\n",
    "            line['subject'] = line['correct']\n",
    "            writer.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
