{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rseetharaman_umass_edu/.conda/envs/rome/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os, re, json\n",
    "import torch, numpy\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "from dsets import CounterFactDataset\n",
    "from util.globals import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/rseetharaman_umass_edu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"/work/pi_dhruveshpate_umass_edu/rseetharaman_umass_edu/llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 21919 elements\n"
     ]
    }
   ],
   "source": [
    "counterfacts = CounterFactDataset(data_dir=DATA_DIR)  # Dataset of known facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_present_in_vocab(vocab, space_token, attr):\n",
    "    for v,idx in vocab.items():\n",
    "\n",
    "        # b'\\\\u0120' - phi-2's and falcon's space token\n",
    "        # b'\\\\u2581' - mistral's and gemma's space token\n",
    "\n",
    "        # Condition to check if the attribute is an atomic token, first unicode character denotes beginning of word.\n",
    "        if v[0].encode(\"unicode_escape\")== space_token and v[1:]==attr:\n",
    "            return (True, idx)\n",
    "    return (False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entries_with_atomic_tokens(tokenizer, space_token, counterfacts):\n",
    "    data = []\n",
    "    for i,cf in enumerate(tqdm(counterfacts)):\n",
    "        is_present, idx = is_present_in_vocab(tokenizer.vocab, space_token, cf['requested_rewrite']['target_new']['str'])\n",
    "        if is_present:\n",
    "            cf.update({\"token_id\": idx})\n",
    "            data.append(cf)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_records(records, model_name):\n",
    "    with jsonlines.open(f\"data_counterfactual_{model_name}.jsonl\", \"w\") as writer:\n",
    "        for d in records:\n",
    "            writer.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"\\u2581Rome\"\n",
    "byte=b'\\\\u2581'\n",
    "s[0].encode(\"unicode_escape\") == byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21919 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [05:42<00:00, 64.02it/s]\n"
     ]
    }
   ],
   "source": [
    "mistral_entries = get_entries_with_atomic_tokens(mistral_tokenizer, b'\\\\u2581', counterfacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_counterfacts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_counterfacts(entries):\n",
    "    counterfactuals = []\n",
    "    for entry in entries:\n",
    "        m = entry['requested_rewrite']\n",
    "        subject = m['subject']\n",
    "        attribute = m['target_new']['str']\n",
    "        full_answer = m['prompt'].format(subject)+ ' {}.'.format(attribute)\n",
    "        prompt = m['prompt'].format(subject)+' '\n",
    "        token_id = entry['token_id']\n",
    "        counterfactuals.append({\n",
    "            \"subject\": attribute,\n",
    "            \"attribute\": attribute,\n",
    "            \"prompt\": prompt,\n",
    "            \"token_id\": token_id,\n",
    "            \"full_answer\": full_answer\n",
    "        })\n",
    "    return counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_counterfacts = prepare_counterfacts(mistral_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i, m in enumerate(mistral_counterfacts):\n",
    "    \n",
    "    all_prompts = random.sample(mistral_counterfacts[:i]+mistral_counterfacts[i+1:], 2)\n",
    "    all_prompts = [a['full_answer'] for a in all_prompts]\n",
    "    all_prompts.append(m['full_answer'])\n",
    "    random.shuffle(all_prompts)\n",
    "    \n",
    "    mistral_counterfacts[i]['original_prompt'] = mistral_counterfacts[i]['prompt']\n",
    "    context = '\\n'.join(all_prompts)\n",
    "\n",
    "    mistral_counterfacts[i]['prompt'] = f\"\"\"Context information is given below.\n",
    "Answer solely based on the context.\n",
    "Context: \n",
    "{context}\n",
    "Question: {m['original_prompt']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is given below.\n",
      "Answer solely based on the context.\n",
      "Context: \n",
      "Palladam is located in the country of Iran.\n",
      "The Waking Eyes, that was started in Seattle.\n",
      "Jari Kurri is a professional soccer.\n",
      "Question: Jari Kurri is a professional \n"
     ]
    }
   ],
   "source": [
    "print(mistral_counterfacts[190]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14679"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistral_counterfacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_records(mistral_counterfacts, 'mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21919 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21919/21919 [05:52<00:00, 62.13it/s]\n"
     ]
    }
   ],
   "source": [
    "llama_entries = get_entries_with_atomic_tokens(llama_tokenizer, b'\\\\u2581', counterfacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13862"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_counterfacts = prepare_counterfacts(llama_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i, m in enumerate(llama_counterfacts):\n",
    "    \n",
    "    all_prompts = random.sample(llama_counterfacts[:i]+llama_counterfacts[i+1:], 2)\n",
    "    all_prompts = [a['full_answer'] for a in all_prompts]\n",
    "    all_prompts.append(m['full_answer'])\n",
    "    random.shuffle(all_prompts)\n",
    "    \n",
    "    llama_counterfacts[i]['original_prompt'] = llama_counterfacts[i]['prompt']\n",
    "    context = '\\n'.join(all_prompts)\n",
    "\n",
    "    llama_counterfacts[i]['prompt'] = f\"\"\"Context information is given below.\n",
    "Answer solely based on the context.\n",
    "Context: \n",
    "{context}\n",
    "Question: {m['original_prompt']}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'mayor',\n",
       " 'attribute': 'mayor',\n",
       " 'prompt': 'Context information is given below.\\nAnswer solely based on the context.\\nContext: \\nBrugmann Mountains is located in Europe.\\nJozef Tomko, who has the position of mayor.\\nBright Promise is to debut on BBC.\\nQuestion: Jozef Tomko, who has the position of ',\n",
       " 'token_id': 9105,\n",
       " 'full_answer': 'Jozef Tomko, who has the position of mayor.',\n",
       " 'original_prompt': 'Jozef Tomko, who has the position of '}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_counterfacts[189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_records(llama_counterfacts, 'llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistral_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
