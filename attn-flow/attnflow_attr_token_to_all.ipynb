{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rseetharaman_umass_edu/.conda/envs/rome/lib/python3.9/site-packages/transformers/utils/generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from attn_contrib import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rseetharaman_umass_edu/.conda/envs/rome/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/rseetharaman_umass_edu/.conda/envs/rome/lib/python3.9/site-packages/transformers/utils/generic.py:342: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f682b8e837401387cb6780fd9f0311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", output_attentions=True, return_dict_in_generate=True,\n",
    "                cache_dir=\"/work/pi_dhruveshpate_umass_edu/rseetharaman_umass_edu/.cache/\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", cache_dir=\"/work/pi_dhruveshpate_umass_edu/rseetharaman_umass_edu/.cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset = json.load(open(\"aug_dataset.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/rseetharaman_umass_edu/.conda/envs/rome/lib/python3.9/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|▏         | 1/50 [00:02<01:57,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|▍         | 2/50 [00:03<01:13,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 3/50 [00:04<00:58,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|▊         | 4/50 [00:05<00:59,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 5/50 [00:06<00:54,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 6/50 [00:07<00:54,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█▍        | 7/50 [00:08<00:48,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|█▌        | 8/50 [00:09<00:42,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 9/50 [00:10<00:41,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 10/50 [00:11<00:38,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|██▏       | 11/50 [00:12<00:36,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▍       | 12/50 [00:13<00:34,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▌       | 13/50 [00:14<00:40,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|██▊       | 14/50 [00:15<00:37,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 15/50 [00:16<00:34,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 16/50 [00:17<00:33,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|███▍      | 17/50 [00:18<00:35,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███▌      | 18/50 [00:19<00:32,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 19/50 [00:20<00:30,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 20/50 [00:21<00:29,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|████▏     | 21/50 [00:22<00:28,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 22/50 [00:23<00:28,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|████▌     | 23/50 [00:24<00:26,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|████▊     | 24/50 [00:25<00:27,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 25/50 [00:26<00:25,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|█████▏    | 26/50 [00:27<00:22,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|█████▍    | 27/50 [00:28<00:21,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 28/50 [00:29<00:21,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|█████▊    | 29/50 [00:30<00:19,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 30/50 [00:31<00:19,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 31/50 [00:32<00:18,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|██████▍   | 32/50 [00:33<00:18,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|██████▌   | 33/50 [00:34<00:16,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 34/50 [00:35<00:15,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 35/50 [00:36<00:15,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|███████▏  | 36/50 [00:37<00:16,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▍  | 37/50 [00:38<00:14,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▌  | 38/50 [00:39<00:12,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|███████▊  | 39/50 [00:40<00:10,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 40/50 [00:41<00:10,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 41/50 [00:43<00:09,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████▍ | 42/50 [00:44<00:08,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|████████▌ | 43/50 [00:45<00:07,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 44/50 [00:46<00:06,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 45/50 [00:47<00:05,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|█████████▏| 46/50 [00:48<00:04,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 47/50 [00:49<00:03,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████▌| 48/50 [00:50<00:02,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|█████████▊| 49/50 [00:51<00:01,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "generations = []\n",
    "for d in tqdm(aug_dataset):\n",
    "    generations.append(get_generation(model, tokenizer, d['prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [d['attribute'] for d in aug_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = []\n",
    "correct = []\n",
    "for i,(a,g) in enumerate(zip(attributes, generations)):\n",
    "    if a.lower() not in g.lower():\n",
    "        incorrect.append(i)\n",
    "    else:\n",
    "        correct.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct)/(len(incorrect)+len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_entries = []\n",
    "for c in correct:\n",
    "    correct_entries.append(aug_dataset[c])\n",
    "\n",
    "incorrect_entries = []\n",
    "for i in incorrect:\n",
    "    incorrect_entries.append(aug_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Nintendo in collaboration with game development tool company AiLive.</s>\n",
      "Layer: 0, top tokens: ['in', 'collaboration', 'accordance', 'sensor', 'designed']\n",
      "Layer: 1, top tokens: ['in', 'Nintendo', 'Nintendo', 'collaboration', 'game']\n",
      "Layer: 2, top tokens: ['game', 'by', 'by', 'by', 'i']\n",
      "Layer: 3, top tokens: ['in', 'collaboration', 'with', 'i', 'company']\n",
      "Layer: 4, top tokens: ['sensor', 'i', 'collaboration', 'in', 'game']\n",
      "Layer: 5, top tokens: ['i', 'Nintendo', 'with', 'in', 'development']\n",
      "Layer: 6, top tokens: ['Nintendo', 'in', 'Nintendo', 'A', 'game']\n",
      "Layer: 7, top tokens: ['in', 'Nintendo', 'collaboration', 'Nintendo', 'Nintendo']\n",
      "Layer: 8, top tokens: ['company', 'A', 'in', 'with', '.']\n",
      "Layer: 9, top tokens: ['s', 'Nintendo', 'in', 'Nintendo', 'uch']\n",
      "Layer: 10, top tokens: ['company', 'with', 'in', '.', 'A']\n",
      "Layer: 11, top tokens: ['in', 'Nintendo', 'collaboration', '.', 'Nintendo']\n",
      "Layer: 12, top tokens: ['by', 'by', 'in', 'Nintendo', 'collaboration']\n",
      "Layer: 13, top tokens: ['in', 'collaboration', 'with', 'Nintendo', 'A']\n",
      "Layer: 14, top tokens: ['Nintendo', 'by', 'with', 'company', 'development']\n",
      "Layer: 15, top tokens: ['Nintendo', 'Nintendo', 'collaboration', 'with', 'Nintendo']\n",
      "Layer: 16, top tokens: ['Nintendo', 'with', 'game', 'collaboration', 'in']\n",
      "Layer: 17, top tokens: ['Nintendo', 'in', 'collaboration', 'by', 'Nintendo']\n",
      "Layer: 18, top tokens: ['by', 'by', 'The', 'by', 'an']\n",
      "Layer: 19, top tokens: ['by', 'by', 'by', 'game', 'is']\n",
      "Layer: 20, top tokens: ['Nintendo', 'by', 'game', 'with', 'Nintendo']\n",
      "Layer: 21, top tokens: ['in', 'with', 'by', 'design', 'the']\n",
      "Layer: 22, top tokens: ['by', 'collaboration', 'is', '.', 'by']\n",
      "Layer: 23, top tokens: ['Nintendo', 'its', 'by', '2', '1']\n",
      "Layer: 24, top tokens: ['by', 'with', 'is', 'developed', '\\n']\n",
      "Layer: 25, top tokens: ['.', 'by', 'Nintendo', 'is', '.']\n",
      "Layer: 26, top tokens: ['by', 'in', 'by', 'with', 'is']\n",
      "Layer: 27, top tokens: ['in', 'by', 'is', 'by', 'Nintendo']\n",
      "Layer: 28, top tokens: ['Nintendo', 'Nintendo', 'by', 'Nintendo', '.']\n",
      "Layer: 29, top tokens: ['in', 'Nintendo', 'Nintendo', 'Nintendo', 'Nintendo']\n",
      "Layer: 30, top tokens: ['Nintendo', 'Nintendo', 'by', 'to', 'Nintendo']\n",
      "Layer: 31, top tokens: ['Nintendo', 'in', 'Nintendo', 'with', 'developed']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[0]['prompt']\n",
    "subject = correct_entries[0]['subject']\n",
    "attribute = correct_entries[0]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oslo.</s>\n",
      "Layer: 0, top tokens: ['lo', '.', 'Os', 'N', 'or']\n",
      "Layer: 1, top tokens: ['lo', 'Os', '.', 'Os', '\\n']\n",
      "Layer: 2, top tokens: ['is', 'is', 'is', 'is', 'is']\n",
      "Layer: 3, top tokens: ['.', 'Os', 'lo', 'way', 'or']\n",
      "Layer: 4, top tokens: ['lo', '.', 'Os', '\\n', 'a']\n",
      "Layer: 5, top tokens: ['Os', '.', 'lo', '0', '\\n']\n",
      "Layer: 6, top tokens: ['Os', 'Os', '.', 'lo', 'Os']\n",
      "Layer: 7, top tokens: ['Os', 'Os', 'Os', 'Os', 'Os']\n",
      "Layer: 8, top tokens: ['.', 'lo', 'Os', 'Os', '']\n",
      "Layer: 9, top tokens: ['Os', 'Os', 'Os', 'lo', 'Os']\n",
      "Layer: 10, top tokens: ['lo', '.', 'Os', 'val', 'of']\n",
      "Layer: 11, top tokens: ['lo', '.', 'Os', 'capital', 'Os']\n",
      "Layer: 12, top tokens: ['lo', 'Os', 'Os', '.', '.']\n",
      "Layer: 13, top tokens: ['lo', '.', 'Os', 'Os', 'Os']\n",
      "Layer: 14, top tokens: ['Os', 'lo', 'Os', 'Os', 'Os']\n",
      "Layer: 15, top tokens: ['Os', 'Os', 'Os', 'Os', 'Os']\n",
      "Layer: 16, top tokens: ['Os', 'in', 'the', 'of', 'capital']\n",
      "Layer: 17, top tokens: ['lo', '.', 'Os', 'Os', 'Os']\n",
      "Layer: 18, top tokens: ['Os', ',', 'in', 'Os', 'the']\n",
      "Layer: 19, top tokens: ['is', 'Os', 'capital', ',', 'capital']\n",
      "Layer: 20, top tokens: ['lo', '.', 'Os', 'Os', 'Os']\n",
      "Layer: 21, top tokens: ['Os', 'is', 'capital', 'capital', 'of']\n",
      "Layer: 22, top tokens: ['Os', 'lo', 'capital', 'with', 'of']\n",
      "Layer: 23, top tokens: ['lo', 'annual', 'University', 'the', 'Os']\n",
      "Layer: 24, top tokens: ['.', 'way', 'lo', ':', 'capital']\n",
      "Layer: 25, top tokens: ['Os', 'is', 'capital', ',', '.']\n",
      "Layer: 26, top tokens: ['An', 'swer', 'capital', 'is', '.']\n",
      "Layer: 27, top tokens: ['lo', 'has', 'a', 'Os', '.']\n",
      "Layer: 28, top tokens: ['.', 'Os', 'lo', 'Os', 'Os']\n",
      "Layer: 29, top tokens: ['lo', '.', '.', 'Os', '.']\n",
      "Layer: 30, top tokens: ['Os', 'Os', 'Os', 'lo', 'Os']\n",
      "Layer: 31, top tokens: ['Os', 'is', ',', '.', 'of']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[1]['prompt']\n",
    "subject = correct_entries[1]['subject']\n",
    "attribute = correct_entries[1]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": History</s>\n",
      "Possibly incorrect: model did not answer correctly\n",
      "Layer: 0, top tokens: ['which', 'may', 'be', 'history', 'regarded']\n",
      "Layer: 1, top tokens: ['which', 'history', 'histor', 'may', 'be']\n",
      "Layer: 2, top tokens: ['ive', 'histor', 'ically', 'book', 'as']\n",
      "Layer: 3, top tokens: ['ive', 'which', 'be', 'definit', 'as']\n",
      "Layer: 4, top tokens: ['may', 'be', 'which', 'regarded', 'as']\n",
      "Layer: 5, top tokens: ['ive', 'as', 'may', 'definit', 'which']\n",
      "Layer: 6, top tokens: ['history', 'ically', 'histor', 'which', 'ive']\n",
      "Layer: 7, top tokens: ['ings', 'be', 'as', 'may', 'which']\n",
      "Layer: 8, top tokens: ['which', 'as', 'history', 'may', 'book']\n",
      "Layer: 9, top tokens: ['as', 'ive', 'which', 'definit', 'its']\n",
      "Layer: 10, top tokens: ['ive', 'which', 'as', '...', 'histor']\n",
      "Layer: 11, top tokens: ['which', 'be', 'ive', 'may', 'as']\n",
      "Layer: 12, top tokens: ['may', 'history', 'be', 'as', 'the']\n",
      "Layer: 13, top tokens: ['which', 'as', 'be', 'ive', 'may']\n",
      "Layer: 14, top tokens: ['history', 'as', 'ive', 'as', 'ically']\n",
      "Layer: 15, top tokens: ['as', 'which', 'the', 'vast', 'be']\n",
      "Layer: 16, top tokens: ['history', 'be', 'may', 'the', 'on']\n",
      "Layer: 17, top tokens: ['which', 'as', 'may', 'regarded', 'ive']\n",
      "Layer: 18, top tokens: ['influenced', 'as', 'other', 'ive', 'definit']\n",
      "Layer: 19, top tokens: ['other', 'which', 'atever', 'Wh', 'history']\n",
      "Layer: 20, top tokens: ['which', 'history', 'as', 'be', 'may']\n",
      "Layer: 21, top tokens: ['may', 'which', 'history', 'be', 'as']\n",
      "Layer: 22, top tokens: ['which', 'may', 'as', 'be', 'other']\n",
      "Layer: 23, top tokens: ['which', 'history', 'ive', 'may', 'regarded']\n",
      "Layer: 24, top tokens: ['ive', 'as', 'a', 'as', 'which']\n",
      "Layer: 25, top tokens: ['history', 'may', 'be', 'ive', 'as']\n",
      "Layer: 26, top tokens: ['which', 'as', 'may', 'history', '...']\n",
      "Layer: 27, top tokens: ['which', 'may', 'be', 'the', 'regarded']\n",
      "Layer: 28, top tokens: ['history', 'which', 'may', 'other', 'a']\n",
      "Layer: 29, top tokens: ['history', 'which', 'ive', 'be', 'regarded']\n",
      "Layer: 30, top tokens: ['history', 'be', 'regarded', 'may', 'vast']\n",
      "Layer: 31, top tokens: ['history', 'which', 'may', 'the', 's']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[2]['prompt']\n",
    "subject = correct_entries[2]['subject']\n",
    "attribute = correct_entries[2]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question solely based on the context.\n",
      "Context:\n",
      "Oxford: Oxford University Press, 2002. PB: ISBN 0-19-818733-5.\n",
      "\n",
      "\n",
      "== External links ==\n",
      "\n",
      "Works by Edward Gibbon in eBook form at Standard Ebooks\n",
      "Works by Edward Gibbon at Project Gutenberg\n",
      "\n",
      " Gibbon, by James Cotter Morison at Project Gutenberg\n",
      "Works by or about Edward Gibbon at Internet Archive\n",
      "Works by Edward Gibbon at LibriVox (public domain audiobooks) \n",
      "Edward Gibbon, Historian of the Roman Empire. Part 1: The Man and his Book\n",
      "Edward Gibbon, Historian of the Roman Empire. Part 2: A closer look at The Decline and Fall (archive link)\n",
      "DeclineandFallResources.com – Original Maps and Footnote Translations\n",
      "Biographer Patricia Craddock's comprehensive bibliography through May 1999 (archived 16 July 2012)\n",
      "Craddock's supplement to her Reference Guide (archived 23 September 2012)\n",
      "Edward Gibbon  (; 8 May 1737 – 16 January 1794) was an English essayist, historian, and politician. His most important work, The History of the Decline and Fall of the Roman Empire, published in six volumes between 1776 and 1789, is known for the quality and irony of its prose, its use of primary sources, and its polemical criticism of organized religion.\n",
      "\n",
      "\n",
      "== Early life: 1737–1752 ==\n",
      "Edward Gibbon was born in 1737, the son of Edward and Judith Gibbon, at Lime Grove in the town of Putney, Surrey. He had five brothers and one sister, all of whom died in infancy.\n",
      "Biography.\n",
      "Dickinson, H. T. \"The Politics of Edward Gibbon\". Literature and History 8:4 (1978), 175–196.\n",
      "Goodall, John (2008), Portchester Castle, London: English Heritage, ISBN 978-1-84802-007-8\n",
      "Low, D. M., Edward Gibbon. 1737–1794 (London: Chatto & Windus, 1937).\n",
      "Murray, John (ed.), The Autobiographies of Edward Gibbon. Second Edition (London: John Murray, 1897).\n",
      "Norton, J. E. A Bibliography of the Works of Edward Gibbon. New York: Burt Franklin Co., 1940, repr. 1970.\n",
      "Norton, J .E. The Letters of Edward Gibbon.\n",
      "Dublin: Edward Posonby.\n",
      "Beer, Gavin de. Gibbon and His World. London: Thames and Hudson, 1968. HB: ISBN 0-670-28981-7.\n",
      "Bowersock, G. W., et al. eds. Edward Gibbon and the Decline and Fall of the Roman Empire. Cambridge, MA: Harvard University Press, 1977.\n",
      "Craddock, Patricia B. Young Edward Gibbon: Gentleman of Letters. Baltimore, MD: Johns Hopkins University Press, 1982. HB: ISBN 0-8018-2714-0. Biography.\n",
      "Jordan, David. Gibbon and his Roman Empire. Urbana, IL: University of Illinois Press, 1971.\n",
      "Keynes, Geoffrey, ed.\n",
      "It is the one English history which may be regarded as definitive...Whatever its shortcomings the book is artistically imposing as well as historically unimpeachable as a vast panorama of a great period.\n",
      "The subject of Gibbon's writing, as well as his ideas and style, have influenced other writers. Besides his influence on Churchill, Gibbon was also a model for Isaac Asimov in his writing of The Foundation Trilogy, which he said involved \"a little bit of cribbin' from the works of Edward Gibbon\".Evelyn Waugh admired Gibbon's style, but not his secular viewpoint.\n",
      "\n",
      "Question:Edward Gibbon's domain of work is the\n",
      "Answer is\n"
     ]
    }
   ],
   "source": [
    "print(correct_entries[2]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antarctica.</s>\n",
      "Layer: 0, top tokens: ['ar', 'ct', 'ica', 'Ant', 'and']\n",
      "Layer: 1, top tokens: ['ar', 'ica', 'ct', 'Ant', 'and']\n",
      "Layer: 2, top tokens: ['ica', 'of', 'ct', 'ar', 'the']\n",
      "Layer: 3, top tokens: ['Arthur', 'Ant', 'ar', 'and', 'ct']\n",
      "Layer: 4, top tokens: ['ica', 'ar', 'Ant', 'Mountains', 'ct']\n",
      "Layer: 5, top tokens: ['Ant', 'ct', 'of', 'ar', 'and']\n",
      "Layer: 6, top tokens: ['Ant', 'An', 'Ant', 'Ant', 'AN']\n",
      "Layer: 7, top tokens: ['Ant', 'Ant', 'Ant', 'Ant', 'Ant']\n",
      "Layer: 8, top tokens: ['ar', 'and', 'ica', 'Ant', 'ct']\n",
      "Layer: 9, top tokens: ['Ant', 'ar', 'Ant', 'An', 'ct']\n",
      "Layer: 10, top tokens: ['ar', 'Ant', 'ica', 'Land', 'ct']\n",
      "Layer: 11, top tokens: ['ar', 'Ant', 'ica', 'Ant', 'Land']\n",
      "Layer: 12, top tokens: ['Ant', 'Ant', 'Ant', 'Ant', 'ar']\n",
      "Layer: 13, top tokens: ['ar', 'ica', 'Land', 'ct', 'Ant']\n",
      "Layer: 14, top tokens: ['Ant', 'ar', 'ct', 'Mountains', 'ights']\n",
      "Layer: 15, top tokens: ['Ant', 'Ant', 'Ant', 'Ant', 'Ant']\n",
      "Layer: 16, top tokens: ['Ant', 'of', 'Ant', 'continent', 'Ant']\n",
      "Layer: 17, top tokens: ['ar', 'ica', 'Ant', 'Ant', 'Ant']\n",
      "Layer: 18, top tokens: ['to', 'Ant', 'of', 'continent', 'ica']\n",
      "Layer: 19, top tokens: ['Ant', 'ar', ',', 'ct', 'of']\n",
      "Layer: 20, top tokens: ['ar', 'Ant', 'ct', 'Ant', 'Ant']\n",
      "Layer: 21, top tokens: ['ar', 'Ant', 'of', 'of', 'on']\n",
      "Layer: 22, top tokens: ['Ant', 'ar', 'ica', 'ac', 'ct']\n",
      "Layer: 23, top tokens: ['Ant', 'of', '\\n', 'of', 'to']\n",
      "Layer: 24, top tokens: ['ar', 'ct', 'is', 'ica', 'of']\n",
      "Layer: 25, top tokens: ['Ant', 'ar', 'and', 'to', 'flows']\n",
      "Layer: 26, top tokens: ['ar', 'Ant', 'of', '\"', ',']\n",
      "Layer: 27, top tokens: ['ar', 'of', 'of', 'Ant', 'ct']\n",
      "Layer: 28, top tokens: ['ar', 'ica', 'Ant', ',', '\\n']\n",
      "Layer: 29, top tokens: ['ica', 'ar', 'ct', 'Ant', 'the']\n",
      "Layer: 30, top tokens: ['ar', 'Ant', 'Ant', 'Ant', 'Ant']\n",
      "Layer: 31, top tokens: ['Ant', 'ar', 'of', 'ct', 'continent']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[3]['prompt']\n",
    "subject = correct_entries[3]['subject']\n",
    "attribute = correct_entries[3]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islam</s>\n",
      "Layer: 0, top tokens: ['\\n', 'K', 'us', 'Islam', 'ail']\n",
      "Layer: 1, top tokens: ['\\n', 'Islam', 'K', '\\n', 'ail']\n",
      "Layer: 2, top tokens: ['\\n', 'and', 'E', 'K', 'Islam']\n",
      "Layer: 3, top tokens: ['\\n', 'K', '\\n', 'Islam', 'Muslim']\n",
      "Layer: 4, top tokens: ['\\n', 'Islam', '\\n', 'religion', 'ail']\n",
      "Layer: 5, top tokens: ['\\n', 'Islam', 'K', '\\n', 'ail']\n",
      "Layer: 6, top tokens: ['Islam', '\\n', 'K', 'Roman', 'us']\n",
      "Layer: 7, top tokens: ['\\n', 'Islam', 'Muslim', '\\n', '\\n']\n",
      "Layer: 8, top tokens: ['Islam', '\\n', 'religion', '\\n', 'us']\n",
      "Layer: 9, top tokens: ['\\n', 'Islam', 'K', 'a', 'ests']\n",
      "Layer: 10, top tokens: ['Islam', '\\n', 'Muslim', 'religion', 'us']\n",
      "Layer: 11, top tokens: ['\\n', 'religion', 'a', 'arly', 'Islam']\n",
      "Layer: 12, top tokens: ['Islam', 'religion', 'of', 'ail', '\\n']\n",
      "Layer: 13, top tokens: ['\\n', 'Islam', 'religion', 'of', 'arly']\n",
      "Layer: 14, top tokens: ['Islam', '\\n', 'a', 'bin', 'ail']\n",
      "Layer: 15, top tokens: ['Islam', '\\n', 'Muslim', 'religion', 'of']\n",
      "Layer: 16, top tokens: ['Islam', '\\n', 'religion', 'E', 'arly']\n",
      "Layer: 17, top tokens: ['\\n', 'Islam', 'K', 'a', 'arly']\n",
      "Layer: 18, top tokens: ['arly', 'Islam', '\\n', 'religion', 'of']\n",
      "Layer: 19, top tokens: ['Islam', '\\n', 'arly', 'religion', 'and']\n",
      "Layer: 20, top tokens: ['Islam', '\\n', 'of', 'arly', 'Muslim']\n",
      "Layer: 21, top tokens: ['Islam', '\\n', 'with', 'of', '\\n']\n",
      "Layer: 22, top tokens: ['\\n', 'Islam', '\\n', 'ail', 'of']\n",
      "Layer: 23, top tokens: ['Islam', '\\n', 'ali', '\\n', 'K']\n",
      "Layer: 24, top tokens: ['\\n', 'Islam', 'with', '\\n', '\\n']\n",
      "Layer: 25, top tokens: ['Islam', '\\n', 'K', 'E', 'a']\n",
      "Layer: 26, top tokens: ['\\n', 'Islam', 'ests', 'K', 'E']\n",
      "Layer: 27, top tokens: ['\\n', 'Islam', 'ail', 'a', '\\n']\n",
      "Layer: 28, top tokens: ['Islam', '\\n', 'of', 'ail', 'arly']\n",
      "Layer: 29, top tokens: ['\\n', 'Islam', 'of', 'ail', 'a']\n",
      "Layer: 30, top tokens: ['Islam', 'ests', 'ail', 'Muslim', '\\n']\n",
      "Layer: 31, top tokens: ['Islam', '\\n', 'ail', 'K', 'a']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[4]['prompt']\n",
    "subject = correct_entries[4]['subject']\n",
    "attribute = correct_entries[4]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": London</s>\n",
      "Layer: 0, top tokens: [',', 'married', 'and', 'London', 'Mer']\n",
      "Layer: 1, top tokens: [',', 'London', 'and', 'married', 'Mer']\n",
      "Layer: 2, top tokens: ['then', 'then', 'London', 'married', 'and']\n",
      "Layer: 3, top tokens: [',', 'and', 'married', 'London', 'in']\n",
      "Layer: 4, top tokens: ['and', ',', 'London', 'married', ' ']\n",
      "Layer: 5, top tokens: [',', 'London', 'and', 'and', 'ington']\n",
      "Layer: 6, top tokens: [',', 'London', 'London', 'London', 'and']\n",
      "Layer: 7, top tokens: [',', 'London', 'London', 'and', 'London']\n",
      "Layer: 8, top tokens: ['and', ',', 'London', 'Street', 'ington']\n",
      "Layer: 9, top tokens: [',', 'and', 'London', 'Mer', 'London']\n",
      "Layer: 10, top tokens: ['London', ',', 'and', 'married', 'Street']\n",
      "Layer: 11, top tokens: [',', 'ington', 'Street', 'and', 'and']\n",
      "Layer: 12, top tokens: ['Street', ',', 'London', 'ington', 'London']\n",
      "Layer: 13, top tokens: [',', 'and', 'London', '.', 'in']\n",
      "Layer: 14, top tokens: ['London', 'ens', 'Street', 'ington', ',']\n",
      "Layer: 15, top tokens: ['Street', 'city', 'of', 'and', ',']\n",
      "Layer: 16, top tokens: ['1', 'on', 'Street', 'in', 'business']\n",
      "Layer: 17, top tokens: [',', 'He', 'and', 'London', 'and']\n",
      "Layer: 18, top tokens: ['G', 'on', 'am', 'a', 'business']\n",
      "Layer: 19, top tokens: [',', 'in', 'retired', '1', 'He']\n",
      "Layer: 20, top tokens: ['London', 'in', ',', 'married', 'and']\n",
      "Layer: 21, top tokens: [',', 'in', 'from', 'from', 'the']\n",
      "Layer: 22, top tokens: [',', 'Street', 'London', 'in', 'and']\n",
      "Layer: 23, top tokens: [',', 'London', 'and', 'from', 'married']\n",
      "Layer: 24, top tokens: ['from', 'in', ',', 'retired', 'from']\n",
      "Layer: 25, top tokens: [',', 'and', 'London', 'retired', 'in']\n",
      "Layer: 26, top tokens: ['London', ',', 'Street', '0', 'and']\n",
      "Layer: 27, top tokens: [',', 'and', 'married', 'in', 'London']\n",
      "Layer: 28, top tokens: [',', 'London', 'from', 'in', 'the']\n",
      "Layer: 29, top tokens: [',', 'and', 'London', '0', 's']\n",
      "Layer: 30, top tokens: ['London', ',', 'and', 'service', '2']\n",
      "Layer: 31, top tokens: ['London', ',', 'in', 'and', 'in']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[5]['prompt']\n",
    "subject = correct_entries[5]['subject']\n",
    "attribute = correct_entries[5]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French</s>\n",
      "Layer: 0, top tokens: ['rench', 'pron', 'unci', 'F', 'ation']\n",
      "Layer: 1, top tokens: ['F', 'rench', ':', 'pron', 'ation']\n",
      "Layer: 2, top tokens: ['1', '(', '(', '(', '];']\n",
      "Layer: 3, top tokens: ['F', 'rench', ':', 'pron', 'unci']\n",
      "Layer: 4, top tokens: ['pron', ':', 'ation', 'unci', '];']\n",
      "Layer: 5, top tokens: ['F', 'G', 'pron', '\"', 'rench']\n",
      "Layer: 6, top tokens: ['F', 'unci', 'rench', 'k', 'pron']\n",
      "Layer: 7, top tokens: ['rench', 'F', 'unci', 'P', 'the']\n",
      "Layer: 8, top tokens: [':', 'rench', 'F', 'mus', 'ation']\n",
      "Layer: 9, top tokens: ['rench', 'F', 'unci', 'ation', 'pron']\n",
      "Layer: 10, top tokens: ['rench', 'pron', 'F', ':', 'rics']\n",
      "Layer: 11, top tokens: ['rench', 'F', '\")', 'unci', '];']\n",
      "Layer: 12, top tokens: ['(', '(', 'rench', 'pron', 'was']\n",
      "Layer: 13, top tokens: ['F', 'rench', 'pron', 'ation', '];']\n",
      "Layer: 14, top tokens: ['F', 'rench', '];', '\")', '(']\n",
      "Layer: 15, top tokens: ['F', 'rench', '];', '\")', 'pron']\n",
      "Layer: 16, top tokens: ['F', '\"', 'rench', 'G', 'If']\n",
      "Layer: 17, top tokens: ['F', 'rench', 'pron', 'unci', 'ation']\n",
      "Layer: 18, top tokens: ['F', 'rench', 'pron', '(', '(']\n",
      "Layer: 19, top tokens: ['rench', 'F', 'pron', 'ation', 'unci']\n",
      "Layer: 20, top tokens: ['F', ':', 'pron', 'rench', 'ation']\n",
      "Layer: 21, top tokens: ['rench', 'ation', 'F', 'unci', ':']\n",
      "Layer: 22, top tokens: ['F', '\"', 'rench', ',', '\\n']\n",
      "Layer: 23, top tokens: ['F', 'rench', 'pron', 'ation', 'unci']\n",
      "Layer: 24, top tokens: ['rench', 'pron', 'ation', '\"', 'unci']\n",
      "Layer: 25, top tokens: ['F', 'rench', 'ation', 'pron', '(']\n",
      "Layer: 26, top tokens: ['F', 'rench', '(', '\\n', '(']\n",
      "Layer: 27, top tokens: ['F', 'rench', 'pron', 'unci', 'ic']\n",
      "Layer: 28, top tokens: ['F', 'rench', 'ation', ':', '];']\n",
      "Layer: 29, top tokens: ['F', 'the', 'was', 'rench', '];']\n",
      "Layer: 30, top tokens: ['F', 'ation', '(', 'rench', '(']\n",
      "Layer: 31, top tokens: ['F', 'ɛ', 'rench', '', 'pron']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[6]['prompt']\n",
    "subject = correct_entries[6]['subject']\n",
    "attribute = correct_entries[6]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Piano.</s>\n",
      "Possibly incorrect: model did not answer correctly\n",
      "Layer: 0, top tokens: ['and', 'play', 'Russian', 'musicians', 'delight']\n",
      "Layer: 1, top tokens: ['and', 'piano', 'piano', 'play', 'musicians']\n",
      "Layer: 2, top tokens: ['play', 'the', 'the', 'the', 'the']\n",
      "Layer: 3, top tokens: ['play', 'and', 'piano', 'song', 'piano']\n",
      "Layer: 4, top tokens: ['play', 'and', 'pian', 'piano', 'began']\n",
      "Layer: 5, top tokens: ['play', 'began', 'to', 'and', 'the']\n",
      "Layer: 6, top tokens: ['play', 'and', 'piano', 'piano', 'to']\n",
      "Layer: 7, top tokens: ['piano', 'piano', 'to', 'and', 'him']\n",
      "Layer: 8, top tokens: ['and', 'play', 'to', 'began', 'piano']\n",
      "Layer: 9, top tokens: ['arrangement', 'of', 'song', 'and', 'piano']\n",
      "Layer: 10, top tokens: ['piano', 'and', 'play', 'began', 'piano']\n",
      "Layer: 11, top tokens: ['and', 'to', 'began', 'play', 'was']\n",
      "Layer: 12, top tokens: ['and', 'piano', 'began', 'piano', 'play']\n",
      "Layer: 13, top tokens: ['began', 'to', 'and', 'piano', 'play']\n",
      "Layer: 14, top tokens: ['piano', 'and', 'to', 'began', 'play']\n",
      "Layer: 15, top tokens: ['to', '\",', 'which', 'began', 'ib']\n",
      "Layer: 16, top tokens: ['piano', 'and', 'time', 'his', 'to']\n",
      "Layer: 17, top tokens: ['and', 'piano', 'began', 'to', 'piano']\n",
      "Layer: 18, top tokens: ['and', 'began', 'to', 'piano', 'play']\n",
      "Layer: 19, top tokens: ['began', 'and', 'to', 'to', 'his']\n",
      "Layer: 20, top tokens: ['piano', 'and', 'began', 'to', 'his']\n",
      "Layer: 21, top tokens: ['the', 'on', 'and', 'piano', '\\n']\n",
      "Layer: 22, top tokens: ['sing', 'piano', 'tone', 'and', 'began']\n",
      "Layer: 23, top tokens: ['piano', 'the', 'a', 'the', 'the']\n",
      "Layer: 24, top tokens: ['began', 'piano', 'to', 'and', 'the']\n",
      "Layer: 25, top tokens: ['piano', 'is', 'and', 'the', 'musicians']\n",
      "Layer: 26, top tokens: ['and', 'to', 'piano', 'the', '—']\n",
      "Layer: 27, top tokens: ['and', 'to', 'began', 'play', 'him']\n",
      "Layer: 28, top tokens: ['piano', 'and', '—', 'to', 'began']\n",
      "Layer: 29, top tokens: ['piano', 'and', 'is', 'the', 'to']\n",
      "Layer: 30, top tokens: ['piano', 'and', 'began', 'musicians', 'him']\n",
      "Layer: 31, top tokens: ['piano', 'and', 'began', 'behind', 'to']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[7]['prompt']\n",
    "subject = correct_entries[7]['subject']\n",
    "attribute = correct_entries[7]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melbourne, Australia.</s>\n",
      "Layer: 0, top tokens: ['Park', 'in', 'Melbourne', 'Melbourne', 'Australia']\n",
      "Layer: 1, top tokens: ['Park', 'Melbourne', 'Melbourne', 'in', 'Melbourne']\n",
      "Layer: 2, top tokens: ['Park', 'from', 'Australia', 'was', 'at']\n",
      "Layer: 3, top tokens: ['Park', 'Melbourne', 'in', 'Australia', 'Melbourne']\n",
      "Layer: 4, top tokens: ['Park', 'in', 'from', ',', 'Melbourne']\n",
      "Layer: 5, top tokens: ['Melbourne', 'Melbourne', 'in', 'Park', ',']\n",
      "Layer: 6, top tokens: ['Melbourne', 'Melbourne', 'Melbourne', 'Australia', 'Melbourne']\n",
      "Layer: 7, top tokens: ['Melbourne', 'Melbourne', 'Melbourne', 'Melbourne', 'Park']\n",
      "Layer: 8, top tokens: ['Melbourne', 'in', 'Park', ',', ',']\n",
      "Layer: 9, top tokens: ['Park', ',', 'Melbourne', 'ong', 'Melbourne']\n",
      "Layer: 10, top tokens: ['Park', 'in', 'Melbourne', 'Melbourne', 'hard']\n",
      "Layer: 11, top tokens: ['Park', ',', 'in', 'Melbourne', 'Melbourne']\n",
      "Layer: 12, top tokens: ['at', 'Park', 'in', 'Melbourne', '']\n",
      "Layer: 13, top tokens: ['in', 'Park', ',', 'Melbourne', 'the']\n",
      "Layer: 14, top tokens: ['Melbourne', 'in', 'the', ',', 'the']\n",
      "Layer: 15, top tokens: ['Melbourne', 'Melbourne', 'Melbourne', 'Melbourne', 'the']\n",
      "Layer: 16, top tokens: ['Melbourne', '7', 'from', '0', 'in']\n",
      "Layer: 17, top tokens: ['Park', 'in', 'Melbourne', 'from', '0']\n",
      "Layer: 18, top tokens: ['in', 'at', 'Melbourne', '', '1']\n",
      "Layer: 19, top tokens: ['at', 'in', '', 'on', 'to']\n",
      "Layer: 20, top tokens: ['Melbourne', 'Park', 'in', 'Melbourne', 'Melbourne']\n",
      "Layer: 21, top tokens: ['Melbourne', 'at', 'Park', '1', 'at']\n",
      "Layer: 22, top tokens: ['at', 'in', 'at', 'Melbourne', '1']\n",
      "Layer: 23, top tokens: ['at', ',', 'at', 'is', 'to']\n",
      "Layer: 24, top tokens: ['at', 'at', 'Park', 'in', 'Melbourne']\n",
      "Layer: 25, top tokens: ['Melbourne', 'is', 'in', 'Melbourne', '0']\n",
      "Layer: 26, top tokens: ['at', 'is', 'Melbourne', ',', 'is']\n",
      "Layer: 27, top tokens: ['Park', 'Melbourne', 'at', 'at', 'is']\n",
      "Layer: 28, top tokens: ['at', 'Melbourne', 'Park', 'in', 'at']\n",
      "Layer: 29, top tokens: ['Park', 'at', ',', 'Melbourne', 'Melbourne']\n",
      "Layer: 30, top tokens: ['Melbourne', 'at', 'Melbourne', 'Park', 'Melbourne']\n",
      "Layer: 31, top tokens: ['Melbourne', 'at', 'Park', '.', 'at']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[8]['prompt']\n",
    "subject = correct_entries[8]['subject']\n",
    "attribute = correct_entries[8]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Chicago Department of Aviation</s>\n",
      "Layer: 0, top tokens: ['O', \"'\", 'H', 'Chicago', 'terminal']\n",
      "Layer: 1, top tokens: ['Chicago', 'O', 'Chicago', 'icago', \"'\"]\n",
      "Layer: 2, top tokens: ['ilton', 'O', 'is', 'Hot', 'are']\n",
      "Layer: 3, top tokens: ['O', 'city', \"'\", 'is', 'are']\n",
      "Layer: 4, top tokens: [\"'\", 'O', 'are', 'ilton', 'is']\n",
      "Layer: 5, top tokens: ['H', \"'\", 'garage', 'terminal', 'O']\n",
      "Layer: 6, top tokens: ['Chicago', 'are', 'H', 'is', 'Chicago']\n",
      "Layer: 7, top tokens: ['Chicago', 'O', 'Chicago', 'Chicago', 'Chicago']\n",
      "Layer: 8, top tokens: ['H', \"'\", 'O', 'are', \"'\"]\n",
      "Layer: 9, top tokens: ['Chicago', 'H', 'are', 'Chicago', 'Airport']\n",
      "Layer: 10, top tokens: ['O', 'are', \"'\", 'H', 'Chicago']\n",
      "Layer: 11, top tokens: ['O', 'ilton', 'It', 'is', 'H']\n",
      "Layer: 12, top tokens: ['O', 'ilton', 'are', \"'\", 'Chicago']\n",
      "Layer: 13, top tokens: ['O', 'hotel', \"'\", 'on', 'are']\n",
      "Layer: 14, top tokens: ['Chicago', 'are', 'ilton', \"'\", 'H']\n",
      "Layer: 15, top tokens: ['Chicago', 'icago', 'Ch', 'Chicago', 'ilton']\n",
      "Layer: 16, top tokens: ['Chicago', 'O', \"'\", 'by', 'H']\n",
      "Layer: 17, top tokens: ['O', \"'\", 'Chicago', 'H', 'are']\n",
      "Layer: 18, top tokens: ['O', \"'\", '–', '2', 'inals']\n",
      "Layer: 19, top tokens: ['O', 'Chicago', 'are', 'parking', 'between']\n",
      "Layer: 20, top tokens: [\"'\", 'O', \"'\", 'are', 'Chicago']\n",
      "Layer: 21, top tokens: ['are', 'O', '2', \"'\", 'Chicago']\n",
      "Layer: 22, top tokens: ['are', 'Chicago', 'is', 'H', 'O']\n",
      "Layer: 23, top tokens: ['are', 'O', 'Chicago', \"'\", 'is']\n",
      "Layer: 24, top tokens: ['the', 'only', 'is', 'between', 'the']\n",
      "Layer: 25, top tokens: ['Chicago', 'is', 'O', '.', \"'\"]\n",
      "Layer: 26, top tokens: ['are', 'Chicago', 'O', 'are', 'the']\n",
      "Layer: 27, top tokens: ['are', 'O', \"'\", 'Chicago', 'H']\n",
      "Layer: 28, top tokens: ['O', 'are', 'Chicago', \"'\", 'H']\n",
      "Layer: 29, top tokens: ['O', 'Chicago', \"'\", 'are', 'on']\n",
      "Layer: 30, top tokens: ['Chicago', 'O', 'are', \"'\", 'International']\n",
      "Layer: 31, top tokens: ['Chicago', 'are', 'O', \"'\", 'H']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[9]['prompt']\n",
    "subject = correct_entries[9]['subject']\n",
    "attribute = correct_entries[9]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe Louis Arena, an arena in Downtown Detroit.</s>\n",
      "Layer: 0, top tokens: ['.', 'Com', 'pleted', 'Detroit', 'in']\n",
      "Layer: 1, top tokens: ['.', 'Detroit', 'Detroit', 'Detroit', 'Detroit']\n",
      "Layer: 2, top tokens: ['own', 'nt', 'Detroit', 'US', 'ia']\n",
      "Layer: 3, top tokens: ['city', '.', 'Detroit', 'arena', '7']\n",
      "Layer: 4, top tokens: ['nt', 'Dow', 'own', 'Cob', 'Detroit']\n",
      "Layer: 5, top tokens: ['7', 'bank', 'nt', 'the', 'own']\n",
      "Layer: 6, top tokens: ['Detroit', 'Detroit', 'Detroit', 'Detroit', 'Detroit']\n",
      "Layer: 7, top tokens: ['Detroit', 'Detroit', 'Detroit', 'Detroit', 'Detroit']\n",
      "Layer: 8, top tokens: ['o', 'Detroit', 'Louis', 'Joe', '.']\n",
      "Layer: 9, top tokens: ['Detroit', 'Detroit', 'Detroit', 'Detroit', 'Detroit']\n",
      "Layer: 10, top tokens: ['in', 'the', '.', 'bank', 'Detroit']\n",
      "Layer: 11, top tokens: ['.', 'Detroit', 'nt', 'over', 'own']\n",
      "Layer: 12, top tokens: ['own', 'Detroit', 'Detroit', 'Detroit', 'Detroit']\n",
      "Layer: 13, top tokens: ['.', 'own', 'to', 'Detroit', '']\n",
      "Layer: 14, top tokens: ['own', 'Detroit', 'nt', 'Dow', 'grew']\n",
      "Layer: 15, top tokens: ['for', 'in', '1', 'Detroit', 'district']\n",
      "Layer: 16, top tokens: ['of', 'Detroit', 'a', 'it', 'was']\n",
      "Layer: 17, top tokens: ['.', 'of', 'own', 'Detroit', 'Detroit']\n",
      "Layer: 18, top tokens: ['9', 'for', 'of', 'to', '7']\n",
      "Layer: 19, top tokens: ['own', 'of', 'pleted', 'in', 'the']\n",
      "Layer: 20, top tokens: ['to', 'the', 'the', 'of', 'Detroit']\n",
      "Layer: 21, top tokens: ['.', 'the', 'of', 'Detroit', 'Com']\n",
      "Layer: 22, top tokens: ['Detroit', 'own', 'of', '.', '2']\n",
      "Layer: 23, top tokens: ['of', 'for', 'at', 'as', 'the']\n",
      "Layer: 24, top tokens: ['to', '.', 'on', 'was', 'was']\n",
      "Layer: 25, top tokens: ['Detroit', '.', 'of', 'Detroit', 'was']\n",
      "Layer: 26, top tokens: ['.', 'the', 'the', 'Detroit', 'the']\n",
      "Layer: 27, top tokens: ['.', 'Detroit', 'in', 'of', 'pleted']\n",
      "Layer: 28, top tokens: ['Detroit', '.', 'own', 'of', 'Detroit']\n",
      "Layer: 29, top tokens: ['.', 'in', 'Detroit', 'at', 'of']\n",
      "Layer: 30, top tokens: ['Detroit', 'Detroit', 'Detroit', 'Detroit', 'own']\n",
      "Layer: 31, top tokens: ['Detroit', 'own', 'nt', 'in', 'for']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[10]['prompt']\n",
    "subject = correct_entries[10]['subject']\n",
    "attribute = correct_entries[10]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question solely based on the context.\n",
      "Context:\n",
      "Joe Louis Arena was an arena in Downtown Detroit. Completed in 1979 at a cost of US$57 million as a replacement for Olympia Stadium, it sat adjacent to Cobo Center on the bank of the Detroit River and was accessible by the Joe Louis Arena station on the Detroit People Mover. The venue was named after former heavyweight champion boxer Joe Louis, who grew up in Detroit.It was the home of the Detroit Red Wings of the National Hockey League and the second oldest NHL venue after Madison Square Garden until the start of the 2017–18 NHL season. Joe Louis Arena was owned by the city of Detroit, and operated by Olympia Entertainment, a subsidiary of team owner Ilitch Holdings.In April 2017, the Red Wings hosted their final game at Joe Louis Arena; the venue was succeeded by Little Caesars Arena.\n",
      "Joe Louis Arena housed 86 premium suites. In 2008, the arena introduced the Comerica Bank Legend's Club, a 181-person private seating location in the arena's southeast corner.\n",
      "\n",
      "\n",
      "=== Replacement and demolition ===\n",
      "\n",
      "On July 20, 2014, following the July 2013 approval of a $650 million project to build a new sports and entertainment district in Downtown Detroit, Christopher Ilitch unveiled designs for a new arena near Comerica Park and Ford Field which was completed in 2017 and succeeded Joe Louis Arena as the home of the Red Wings.\n",
      "Figure Skating Championships, best known for the pre-competition attack on Nancy Kerrigan by associates of Tonya Harding. In addition, Joe Louis Arena was the site of the 2013 edition of the Skate America figure skating competition.On May 7, 2015, it was announced that the Horizon League men's basketball tournament would be held in Detroit beginning in 2016 under a five-year deal; the 2016 and 2017 tournaments were held at Joe Louis Arena.On December 4, 2016, Joe Louis Arena hosted its final OHL game as the Windsor Spitfires defeated the Saginaw Spirit 3–2.On February 10, 2017, Joe Louis Arena hosted its final regular season college hockey game as the Michigan Wolverines defeated the Michigan State Spartans 5–4 in a shootout.Joe Louis Arena was also a concert venue.\n",
      "Four of the games (ArenaBowl III, ArenaBowl IV, ArenaBowl V and ArenaBowl VII) were played at Joe Louis Arena.WWE hosted numerous shows at the arena, including the Survivor Series pay-per-view in 1991, 1999 and 2005.Joe Louis Arena hosted the 1994 U.S. Figure Skating Championships, best known for the pre-competition attack on Nancy Kerrigan by associates of Tonya Harding.\n",
      "Until the Palace opened in 1988, Joe Louis Arena was Michigan's largest indoor arena for concerts. The first concert to take place there occurred on February 17, 1980, in which Max Webster opened for the Canadian rock group Rush. This venue was used for the Alice Cooper concert film The Nightmare Returns in 1986. The last concert at the venue was Summer Jamz 20! on July 23, 2017.\n",
      "\n",
      "\n",
      "== In popular culture ==\n",
      "The arena is featured in the movie Straight Outta Compton in a scene depicting N.W.A's performance of their controversial song \"Fuck tha Police\".\n",
      "\n",
      "\n",
      "== References ==\n",
      "\n",
      "Question:Joe Louis Arena, from the\n",
      "Answer is\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon.</s>\n",
      "Layer: 0, top tokens: ['Fire', 'Fire', 'called', 'Kind', 'tablet']\n",
      "Layer: 1, top tokens: ['Fire', 'Amazon', 'Amazon', ',', 'Amazon']\n",
      "Layer: 2, top tokens: ['Kind', 'le', 'Fire', 'The', 'The']\n",
      "Layer: 3, top tokens: ['Fire', ',', 'is', 'tablet', 'a']\n",
      "Layer: 4, top tokens: ['Kind', ',', 'the', 'is', 'Amazon']\n",
      "Layer: 5, top tokens: ['Fire', 'the', ',', 'is', 'Amazon']\n",
      "Layer: 6, top tokens: ['Amazon', 'Kind', 'Fire', ',', 'Amazon']\n",
      "Layer: 7, top tokens: ['Amazon', 'Fire', 'Amazon', ',', 'Amazon']\n",
      "Layer: 8, top tokens: ['Fire', 'Kind', ',', 'the', 'le']\n",
      "Layer: 9, top tokens: ['Fire', 'by', 'Kind', 'le', 'Amazon']\n",
      "Layer: 10, top tokens: [',', 'Fire', 'is', 'Kind', 'Amazon']\n",
      "Layer: 11, top tokens: ['Fire', 'by', 'line', 'le', 'Kind']\n",
      "Layer: 12, top tokens: ['Kind', 'the', 'The', 'Fire', 'Amazon']\n",
      "Layer: 13, top tokens: ['Amazon', 'Fire', 'the', 'of', 'formerly']\n",
      "Layer: 14, top tokens: ['Amazon', 'Fire', 'by', 'Kind', ',']\n",
      "Layer: 15, top tokens: [',', 'Amazon', 'by', 'the', 'Amazon']\n",
      "Layer: 16, top tokens: ['Fire', 'Amazon', ',', 'the', 'by']\n",
      "Layer: 17, top tokens: ['Fire', ',', 'is', 'the', '.']\n",
      "Layer: 18, top tokens: ['The', 'by', 'Amazon', 'the', 'to']\n",
      "Layer: 19, top tokens: ['by', 'Fire', 'computers', 'the', 'called']\n",
      "Layer: 20, top tokens: ['Fire', 'Amazon', ',', 'by', 'formerly']\n",
      "Layer: 21, top tokens: ['2', 'The', 'Fire', '2', 'The']\n",
      "Layer: 22, top tokens: ['The', 'The', '2', '2', 'The']\n",
      "Layer: 23, top tokens: ['Fire', 'Amazon', ',', ',', 'Qu']\n",
      "Layer: 24, top tokens: ['The', 'Fire', 'Amazon', ',', 'The']\n",
      "Layer: 25, top tokens: [',', 'Amazon', 'is', 'by', 'the']\n",
      "Layer: 26, top tokens: [',', 'Amazon', 'The', 'Fire', 'the']\n",
      "Layer: 27, top tokens: ['Fire', 'Amazon', ',', 'the', 'formerly']\n",
      "Layer: 28, top tokens: ['Amazon', 'Fire', 'is', ',', 'le']\n",
      "Layer: 29, top tokens: ['Amazon', ',', 'Fire', 'formerly', 'the']\n",
      "Layer: 30, top tokens: ['Amazon', 'the', 'Fire', ',', 'formerly']\n",
      "Layer: 31, top tokens: ['Amazon', 'The', 'Fire', 'the', 'Amazon']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[11]['prompt']\n",
    "subject = correct_entries[11]['subject']\n",
    "attribute = correct_entries[11]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question solely based on the context.\n",
      "Context:\n",
      "The Amazon Fire, formerly called the Kindle Fire, is a line of tablet computers developed by Amazon. Built with Quanta Computer, the Kindle Fire was first released in November 2011, featuring a color 7-inch multi-touch display with IPS technology and running on Fire OS, an Android-based operating system. The Kindle Fire HD followed in September 2012, and the Kindle Fire HDX in September 2013. In September 2014, when the fourth generation was introduced, the name \"Kindle\" was dropped. In later generations, the Fire tablet is also able to convert into a Smart speaker turning on the \"Show Mode\" options, which the primary interaction will be by voice command through Alexa.\n",
      "Though the tablet still features budget hardware Amazon increased the base price to $59.99.\n",
      "\n",
      "\n",
      "== Design ==\n",
      "\n",
      "\n",
      "=== Hardware ===\n",
      "The Kindle Fire hardware is manufactured by Quanta Computer (an Original Design Manufacturer), which also originally helped design the BlackBerry PlayBook, using it as a hardware template for the Kindle Fire.\n",
      "First-generation Kindle Fire devices employed a 1-GHz Texas Instruments OMAP 4430 dual-core processor. The device has a 2-point multi-touch colour LCD screen with a diagonal length of 7 inches (180 mm) and a 600×1024-pixel resolution (160 dpi density). Connectivity is through 802.11n Wi-Fi and USB 2.0 (Micro-B connector).\n",
      "== History ==\n",
      "The Kindle Fire—which includes access to the Amazon Appstore, streaming movies and TV shows, and the Kindle Store for e-books—was released to consumers in the United States on November 14, 2011, after being announced on September 28.The original Kindle Fire retailed for US$199 in 2011.\n",
      "Estimates of the device's initial bill of materials cost ranged from $150 to $202. Amazon's business strategy was stated in 2011 as making money through sales of digital content on the Fire, rather than through sales of the device itself.On September 6, 2012, the Kindle Fire was upgraded to the second generation, and its price was reduced to US$159, RAM upgraded to 1 GB and processor clock speed upgraded to 1.2 GHz.\n",
      "On September 7, 2012, upgrades to the device were announced with consumer availability to those European countries with a localised version of Amazon's website (United Kingdom, France, Germany, Italy and Spain).As of October 2012, the Kindle Fire was the second best selling tablet after Apple's iPad, with about 7 million units sold according to estimates by Forrester Research and as of 2013 Amazon's tablets were the fourth best selling.The Fire tablet line was not updated until 2015; Amazon only released Fire HD and Fire HDX tablets during that time. In 2015 Amazon made a full refresh of their tablet family where they brought the range down market as a series of budget focused devices, returning to the lower-spec Fire line and cancelling the HDX line.\n",
      "With this development the company aims to introduce new user features such as a system-wide dark mode.\n",
      "\n",
      "\n",
      "== Reception ==\n",
      "Analysts had projected the device to be a strong competitor to Apple's iPad, and that other Android device makers would suffer lost sales.In a 2012 review published by Project Gutenberg, the Kindle Fire was called a \"huge step back in freedom from the Kindle 3\"; the reviewer noted that Amazon introduced a \"deliberate limitation\" into the Fire that didn't exist in the previous version: it is no longer possible to download free e-books from websites such as Project Gutenberg, Internet Archive and Google Books and have them stored permanently in the same places where books from Amazon are kept.\n",
      "\n",
      "Question:Kindle Fire, created by\n",
      "Answer is\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":French</s>\n",
      "Layer: 0, top tokens: ['singer', '-', 'ong', 's', 'French']\n",
      "Layer: 1, top tokens: ['singer', 'France', 'French', 'writer', '-']\n",
      "Layer: 2, top tokens: ['Journal', 'An', 'l', 'My', 'La']\n",
      "Layer: 3, top tokens: ['writer', 'singer', 'ong', '-', 'My']\n",
      "Layer: 4, top tokens: ['writer', 'My', 'singer', 'l', 'France']\n",
      "Layer: 5, top tokens: ['ong', 'singer', 'writer', 'French', '-']\n",
      "Layer: 6, top tokens: ['ong', 'singer', 'France', 'French', 'TF']\n",
      "Layer: 7, top tokens: ['writer', 'France', 'ong', 'singer', 'French']\n",
      "Layer: 8, top tokens: ['writer', 'singer', '-', 'ît', 'ong']\n",
      "Layer: 9, top tokens: ['-', 'French', 'indeed', ':', 'writer']\n",
      "Layer: 10, top tokens: ['singer', 'writer', '-', 'in', 'mer']\n",
      "Layer: 11, top tokens: ['singer', 'mer', 'Far', 'the', 'ong']\n",
      "Layer: 12, top tokens: ['singer', 'writer', '2', 'French', 'My']\n",
      "Layer: 13, top tokens: ['writer', 'singer', 'French', '-', 'My']\n",
      "Layer: 14, top tokens: ['French', '-', 'ong', 'singer', 'My']\n",
      "Layer: 15, top tokens: ['', '', 'writer', 'on', 'Far']\n",
      "Layer: 16, top tokens: ['French', 's', '-', '\\n', 'singer']\n",
      "Layer: 17, top tokens: ['singer', 'French', '-', 'writer', 'ong']\n",
      "Layer: 18, top tokens: ['-', 'in', 'French', 'writer', 'singer']\n",
      "Layer: 19, top tokens: ['-', 'French', 'singer', 'ong', 'writer']\n",
      "Layer: 20, top tokens: ['French', 'ong', '-', 'singer', '2']\n",
      "Layer: 21, top tokens: ['ong', '-', 'French', 'singer', 'mer']\n",
      "Layer: 22, top tokens: ['2', '-', 'is', 'ong', '.']\n",
      "Layer: 23, top tokens: ['French', 'ong', 'mer', '-', '.']\n",
      "Layer: 24, top tokens: ['is', '-', 'singer', 'writer', 'as']\n",
      "Layer: 25, top tokens: ['French', 'becoming', 'with', 'on', 'mer']\n",
      "Layer: 26, top tokens: ['ong', 'French', '-', 'singer', 'writer']\n",
      "Layer: 27, top tokens: ['-', 'ong', 's', 'singer', 'French']\n",
      "Layer: 28, top tokens: ['French', 'in', 'is', 'in', 'writer']\n",
      "Layer: 29, top tokens: ['-', 'singer', 'ong', 'French', 'the']\n",
      "Layer: 30, top tokens: ['French', '-', 'ong', 'singer', '1']\n",
      "Layer: 31, top tokens: ['French', '-', 'singer', 'the', '\"']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[12]['prompt']\n",
    "subject = correct_entries[12]['subject']\n",
    "attribute = correct_entries[12]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Olga Vittoria Gentilli writes in Italian.</s>\n",
      "Layer: 0, top tokens: ['stage', 'film', 'and', 'Italian', 'war']\n",
      "Layer: 1, top tokens: ['stage', 'Italian', 'Italian', 'and', 'Italian']\n",
      "Layer: 2, top tokens: ['an', 'asc', 'Nap', 'Italian', 'ina']\n",
      "Layer: 3, top tokens: ['actress', 'stage', 'film', 'and', '.']\n",
      "Layer: 4, top tokens: ['the', 'actress', 'C', 'F', 'war']\n",
      "Layer: 5, top tokens: ['.', 'ist', '3', 'asc', 'films']\n",
      "Layer: 6, top tokens: ['Italian', 'asc', 'Nap', 'Italian', 'stage']\n",
      "Layer: 7, top tokens: ['Italian', 'Italian', 'Italian', 'Italian', 'Italian']\n",
      "Layer: 8, top tokens: ['in', 'and', 'Italian', 'stage', 'ina']\n",
      "Layer: 9, top tokens: ['She', 'Italian', 'asc', 'ano', 'Italian']\n",
      "Layer: 10, top tokens: ['and', '.', 'Italian', 'Italian', 'asc']\n",
      "Layer: 11, top tokens: ['stage', 'ia', 'Nap', 'era', 'asc']\n",
      "Layer: 12, top tokens: ['an', 'film', '1', 'stage', 'Italian']\n",
      "Layer: 13, top tokens: ['Italian', 'stage', 'She', 'actress', 'film']\n",
      "Layer: 14, top tokens: ['Italian', 'an', 'is', 'stage', 'and']\n",
      "Layer: 15, top tokens: ['1', 'ill', 'actress', 'She', 'as']\n",
      "Layer: 16, top tokens: ['Italian', 'an', 'stage', 'is', 'and']\n",
      "Layer: 17, top tokens: ['stage', 'actress', 'film', 'an', '.']\n",
      "Layer: 18, top tokens: ['an', 'actress', 'as', 'film', 'Italian']\n",
      "Layer: 19, top tokens: ['an', 'stage', 'is', 'film', 'Italian']\n",
      "Layer: 20, top tokens: ['an', 'and', 'Italian', 'stage', 'film']\n",
      "Layer: 21, top tokens: ['stage', 'actress', 'film', 'Italian', 'in']\n",
      "Layer: 22, top tokens: ['in', 'film', 'in', 'an', 'and']\n",
      "Layer: 23, top tokens: ['Italian', 'an', 'film', 'stage', 'and']\n",
      "Layer: 24, top tokens: ['Italian', 'stage', 'in', 'appeared', 'in']\n",
      "Layer: 25, top tokens: ['is', 'film', 'Italian', 'She', ')']\n",
      "Layer: 26, top tokens: ['film', 'Italian', 'stage', 'actress', 'and']\n",
      "Layer: 27, top tokens: ['stage', 'film', '\\n', 'and', 'An']\n",
      "Layer: 28, top tokens: ['stage', 'an', 'Italian', 'is', 'film']\n",
      "Layer: 29, top tokens: ['stage', 'film', 'Italian', 'and', 'actress']\n",
      "Layer: 30, top tokens: ['Italian', 'stage', 'and', 'an', 'Italian']\n",
      "Layer: 31, top tokens: ['Italian', 'and', 'stage', 'played', 'in']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[13]['prompt']\n",
    "subject = correct_entries[13]['subject']\n",
    "attribute = correct_entries[13]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India.</s>\n",
      "Layer: 0, top tokens: ['.', '\\n', 'India', '==', '==']\n",
      "Layer: 1, top tokens: ['.', 'India', 'Indian', '\\n', '\\n']\n",
      "Layer: 2, top tokens: [',', ',', ',', ',', ',']\n",
      "Layer: 3, top tokens: ['.', '\\n', '\\n', '\\n', 'India']\n",
      "Layer: 4, top tokens: ['.', '\\n', 'India', 'arn', 'K']\n",
      "Layer: 5, top tokens: ['India', 'is', '.', 'a', 'in']\n",
      "Layer: 6, top tokens: ['France', 'India', 'Portugal', 'Indian', 'oc']\n",
      "Layer: 7, top tokens: ['India', '.', 'Indian', '\\n', '\\n']\n",
      "Layer: 8, top tokens: ['France', 'India', '.', '\\n', 'Italian']\n",
      "Layer: 9, top tokens: ['Indian', '\\n', '1', 'India', '\\n']\n",
      "Layer: 10, top tokens: ['France', 'of', 'country', 'Italian', 'Finland']\n",
      "Layer: 11, top tokens: ['.', 'India', 'country', 'arn', '\\n']\n",
      "Layer: 12, top tokens: ['country', 'Italian', 'of', ',', 'India']\n",
      "Layer: 13, top tokens: ['.', ',', 'in', 'country', '\\n']\n",
      "Layer: 14, top tokens: ['India', 'country', 'y', 'of', 'in']\n",
      "Layer: 15, top tokens: ['y', ',', 'Indian', 'country', 'of']\n",
      "Layer: 16, top tokens: ['of', 'country', 'y', 'India', 'K']\n",
      "Layer: 17, top tokens: ['.', 'of', 'country', '\\n', 'India']\n",
      "Layer: 18, top tokens: ['India', 'country', ',', 'of', 'aka']\n",
      "Layer: 19, top tokens: ['country', 'of', 'India', 'is', 'ta']\n",
      "Layer: 20, top tokens: ['of', 'is', 'India', ',', '\\n']\n",
      "Layer: 21, top tokens: ['from', 'of', 'India', 'from', '.']\n",
      "Layer: 22, top tokens: ['.', 'India', 'of', 'country', 'is']\n",
      "Layer: 23, top tokens: ['.', 'India', '\\n', 'ar', 'K']\n",
      "Layer: 24, top tokens: ['his', 'such', 'India', '.', 'of']\n",
      "Layer: 25, top tokens: ['.', 'India', '\\n', 'is', 'aka']\n",
      "Layer: 26, top tokens: ['.', 'y', 'India', '\\n', 'of']\n",
      "Layer: 27, top tokens: ['.', 'India', 'ar', 'in', 'in']\n",
      "Layer: 28, top tokens: ['India', 'of', 'in', '.', 'country']\n",
      "Layer: 29, top tokens: ['.', 'India', 'of', 'country', 'beautiful']\n",
      "Layer: 30, top tokens: ['India', 'Finland', '.', 'country', 'Netherlands']\n",
      "Layer: 31, top tokens: ['India', 'of', '.', ',', 'country']\n"
     ]
    }
   ],
   "source": [
    "prompt = correct_entries[14]['prompt']\n",
    "subject = correct_entries[14]['subject']\n",
    "attribute = correct_entries[14]['attribute']\n",
    "\n",
    "get_focus_tokens_from(model, tokenizer, prompt, subject=attribute, attribute=attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
